{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5721bcc64724c789bdd1b45a87e8bdc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explain the theory of relativity in simple terms. A: Einstein's theory of relativity is a scientific theory that explains how the laws of physics are the same for all observers in uniform motion relative to one another. It consists of two main components: special relativity and general relativity. Special relativity explains how time and space are affected by motion, while general relativity explains how gravity affects the curvature of spacetime. B: The theory of relativity is a fundamental concept in modern\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# Define the model name\n",
    "model_name = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "\n",
    "# Load the tokenizer and model from local cache in FP16\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, local_files_only=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    local_files_only=True,\n",
    "    torch_dtype=torch.float16,  # Load in FP16 precision\n",
    "    device_map=\"auto\"           # Automatically map layers to available devices\n",
    ")\n",
    "\n",
    "# Define a prompt\n",
    "prompt = \"Explain the theory of relativity in simple terms.\"\n",
    "\n",
    "# Tokenize and prepare input for the model\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Get the device of the model's embeddings (first parameter)\n",
    "embedding_device = next(iter(model.parameters())).device\n",
    "\n",
    "# Move inputs to the device of the model's embeddings\n",
    "inputs = {key: value.to(embedding_device) for key, value in inputs.items()}\n",
    "\n",
    "# Generate the output with customized parameters\n",
    "outputs = model.generate(\n",
    "    inputs[\"input_ids\"],\n",
    "    attention_mask=inputs[\"attention_mask\"],  # Ensure attention mask is also on the correct device\n",
    "    max_length=100,\n",
    "    num_return_sequences=1,\n",
    "    temperature=0.7,\n",
    "    top_k=50,\n",
    "    top_p=0.9,\n",
    "    do_sample=True,\n",
    "    pad_token_id=tokenizer.eos_token_id       # EOS token padding for proper generation\n",
    ")\n",
    "\n",
    "# Decode and print the generated output\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: The applications of AI include expert systems, natural language processing, speech recognition, and machine vision. \n",
      "These applications involve simulating human intelligence processes by machines, especially computer systems. They enable machines\n",
      "to learn, reason, and self-correct, making them useful in various fields. Expert systems are used to mimic human expertise,\n",
      "natural language processing is used to understand and generate human language, speech recognition is used to recognize spoken\n",
      "commands or words, and machine vision is used to enable computers to see and understand visual data. \n",
      "\n",
      "Reasoning Skill: Identifying Pros And Cons\n",
      "In this question, the reasoning skill of Identifying Pros And Cons is relevant because the question asks for the applications of\n",
      "AI, which involves identifying the benefits and advantages of AI in different fields. To answer this question correctly, one needs\n",
      "to weigh\n"
     ]
    }
   ],
   "source": [
    "# Define the context and question\n",
    "context = \"\"\"Artificial Intelligence (AI) is the simulation of human intelligence processes by machines, especially computer systems.\n",
    "These processes include learning (the acquisition of information and rules for using it), reasoning (using rules to reach approximate\n",
    "or definite conclusions), and self-correction. AI applications include expert systems, natural language processing, speech recognition,\n",
    "and machine vision.\"\"\"\n",
    "\n",
    "question = \"What are some applications of Artificial Intelligence?\"\n",
    "\n",
    "# Create the prompt for question answering\n",
    "prompt = f\"Context:\\n{context}\\n\\nQuestion:\\n{question}\\n\\nAnswer:\"\n",
    "\n",
    "# Tokenize and prepare input for the model\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Get the device of the model's embeddings (first parameter)\n",
    "embedding_device = next(iter(model.parameters())).device\n",
    "\n",
    "# Move inputs to the device of the model's embeddings\n",
    "inputs = {key: value.to(embedding_device) for key, value in inputs.items()}\n",
    "\n",
    "# Generate the answer\n",
    "outputs = model.generate(\n",
    "    inputs[\"input_ids\"],\n",
    "    attention_mask=inputs[\"attention_mask\"],\n",
    "    max_length=250,\n",
    "    temperature=0.7,\n",
    "    top_k=50,\n",
    "    top_p=0.9,\n",
    "    do_sample=True,\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "# Decode and print the answer\n",
    "answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "# Extract the answer portion after the prompt\n",
    "answer = answer[len(prompt):].strip()\n",
    "print(f\"Answer: {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis: The text is an expression of extreme satisfaction and happiness. The use of the word \"happier\" instead of \"happy\" suggests a strong emotional intensity, implying that the speaker is extremely pleased. The phrase \"absolutely perfect\" reinforces this sentiment, conveying a sense of completeness and satisfaction. The tone of the text is overwhelmingly positive, indicating that the speaker is extremely content with their experience.\n",
      "\n",
      "Sentiment: Positive\n",
      "\n",
      "Reasoning:\n",
      "* The use of superlatives such as \"happier\" and \"absolutely perfect\" suggests a strong emotional intensity and reinforces the positive sentiment.\n",
      "* The text does not contain any negative language or words that could indicate a negative sentiment.\n",
      "* The overall tone of the text is one of extreme satisfaction and happiness, indicating a positive sentiment.  | \n",
      "---\n",
      "\n",
      "Determine the sentiment of the\n"
     ]
    }
   ],
   "source": [
    "# Text for sentiment analysis\n",
    "text = \"I couldn't be happier with the service I received today. Everything was absolutely perfect!\"\n",
    "\n",
    "# Create the prompt for sentiment analysis\n",
    "prompt = f\"Determine the sentiment of the following text and explain your reasoning:\\n{text}\\n\\nAnalysis:\"\n",
    "\n",
    "# Tokenize and prepare input\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "embedding_device = next(iter(model.parameters())).device\n",
    "inputs = {key: value.to(embedding_device) for key, value in inputs.items()}\n",
    "\n",
    "# Generate the analysis\n",
    "outputs = model.generate(\n",
    "    inputs[\"input_ids\"],\n",
    "    attention_mask=inputs[\"attention_mask\"],\n",
    "    max_length=200,\n",
    "    temperature=0.7,\n",
    "    top_k=50,\n",
    "    top_p=0.9,\n",
    "    do_sample=True,\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "# Decode and print the analysis\n",
    "analysis = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "# Extract the analysis portion after the prompt\n",
    "analysis = analysis[len(prompt):].strip()\n",
    "print(f\"Analysis: {analysis}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translation: Bonjour, comment allez-vous aujourd'hui?\n",
      "\n",
      "Hello, how are you today? (in a more informal way)\n",
      "Bonjour, comment ça va?\n",
      "\n",
      "Hello, how are you today? (in a more formal way)\n",
      "Bonjour, comment allez-vous?\n",
      "\n",
      "If you want to ask how someone is doing in a more polite way, you can use the phrase: \n",
      "Comment allez-vous, je m'appelle [\n"
     ]
    }
   ],
   "source": [
    "# Define the text to translate\n",
    "text_to_translate = \"Hello, how are you today?\"\n",
    "\n",
    "# Define the translation prompt\n",
    "prompt = f\"Translate the following text to French:\\n{text_to_translate}\\n\\nTranslation:\"\n",
    "\n",
    "# Tokenize and prepare input for the model\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "embedding_device = next(iter(model.parameters())).device\n",
    "\n",
    "# Move inputs to the device of the model's embeddings\n",
    "inputs = {key: value.to(embedding_device) for key, value in inputs.items()}\n",
    "\n",
    "# Generate the translation\n",
    "outputs = model.generate(\n",
    "    inputs[\"input_ids\"],\n",
    "    attention_mask=inputs[\"attention_mask\"],\n",
    "    max_length=100,\n",
    "    temperature=0.7,\n",
    "    top_k=50,\n",
    "    top_p=0.9,\n",
    "    do_sample=True,\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "# Decode and print the translation\n",
    "translation = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "# Extract the translation portion after the prompt\n",
    "translation = translation[len(prompt):].strip()\n",
    "print(f\"Translation: {translation}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solution:\n",
      "Step 1: Identify the problem type\n",
      "This problem is a distance problem, where we need to calculate the distance traveled by a train.\n",
      "\n",
      "Step 2: Identify the given information\n",
      "The given information is the speed of the train (60 miles per hour) and the time it travels (2.5 hours).\n",
      "\n",
      "Step 3: Recall the formula for distance\n",
      "The formula for distance is: distance = speed × time.\n",
      "\n",
      "Step 4: Plug in the values into the formula\n",
      "distance = 60 miles per hour × 2.5 hours\n",
      "\n",
      "Step 5: Multiply the values\n",
      "distance = 150 miles\n",
      "\n",
      "Step 6: Write the final answer\n",
      "The train travels 150 miles.\n",
      "\n",
      "Explanation of each step:\n",
      "Step 1: Identify the problem type\n",
      "The problem is a distance problem because it asks for the distance traveled by the train.\n",
      "\n",
      "Step 2: Identify the given information\n",
      "We are given the speed of the train (60 miles per hour) and the time it travels (2.5 hours).\n",
      "\n",
      "Step 3:\n"
     ]
    }
   ],
   "source": [
    "# Define the mathematical problem\n",
    "problem = \"If a train travels at a speed of 60 miles per hour for 2.5 hours, how far does it travel?\"\n",
    "\n",
    "# Create the prompt for chain-of-thought reasoning\n",
    "prompt = f\"Solve the following problem and explain each step:\\n{problem}\\n\\nSolution:\"\n",
    "\n",
    "# Tokenize and prepare input for the model\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "embedding_device = next(iter(model.parameters())).device\n",
    "\n",
    "# Move inputs to the device of the model's embeddings\n",
    "inputs = {key: value.to(embedding_device) for key, value in inputs.items()}\n",
    "\n",
    "# Generate the solution\n",
    "outputs = model.generate(\n",
    "    inputs[\"input_ids\"],\n",
    "    attention_mask=inputs[\"attention_mask\"],\n",
    "    max_length=250,\n",
    "    temperature=0.7,\n",
    "    top_k=50,\n",
    "    top_p=0.9,\n",
    "    do_sample=True,\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "# Decode and print the solution\n",
    "solution = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "# Extract the solution portion after the prompt\n",
    "solution = solution[len(prompt):].strip()\n",
    "print(f\"Solution:\\n{solution}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
